{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d3b04218-0413-4e6c-8751-5d8a404d73a9",
    "_uuid": "0bca9739b82d5d51e1229243e03ea1b6db35c17e"
   },
   "source": [
    "## Introduction\n",
    "\n",
    "This kernel shows how to use NBSVM (Naive Bayes - Support Vector Machine) to create a strong baseline. In this kernel, we use sklearn's logistic regression, rather than SVM, although in practice the two are nearly identical (sklearn uses the liblinear library behind the scenes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "ef06cd19-66b6-46bc-bf45-184e12d3f7d4",
    "_uuid": "cca038ca9424a3f66e10262fc9129de807b5f855",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import re, string"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a494f561-0c2f-4a38-8973-6b60c22da357",
    "_uuid": "f70ebe669fcf6b434c595cf6fb7a76120bf7809c"
   },
   "source": [
    "## Take Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "a494f561-0c2f-4a38-8973-6b60c22da357",
    "_uuid": "f70ebe669fcf6b434c595cf6fb7a76120bf7809c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the train and test data\n",
    "def readInputFiles(train_file_path, test_file_path):\n",
    "    train = pd.read_csv(train_file_path)\n",
    "    test = pd.read_csv(test_file_path)\n",
    "    return train, test\n",
    "    \n",
    "train, test = readInputFiles('../dataset/train_new.csv', '../dataset/test_new.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "3996a226-e1ca-4aa8-b39f-6524d4dadb07",
    "_uuid": "2c18461316f17d1d323b1959c8eb4e5448e8a44e"
   },
   "source": [
    "## Data Exploration"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "5ddb337b-c9b2-4fec-9652-cb26769dc3c6",
    "_uuid": "5f5269c56ea6ded273881b0d4dcdb6af83a3e089",
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Grandma Terri Should Burn in Trash \n",
      "Grandma Terri is trash. I hate Grandma Terri. F%%K her to HELL! 71.74.76.40\n",
      "\n",
      "Average length of comments: 395\n",
      "Standard dev: 594\n",
      "Longest Comment: 5895\n",
      "\n",
      "# Comments in train data: 127656 \n",
      "# Comments in test data: 31915\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>140030</td>\n",
       "      <td>ed56f082116dcbd0</td>\n",
       "      <td>Grandma Terri Should Burn in Trash \\nGrandma T...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>159124</td>\n",
       "      <td>f8e3cd98b63bf401</td>\n",
       "      <td>, 9 May 2009 (UTC)\\nIt would be easiest if you...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60006</td>\n",
       "      <td>a09e1bcf10631f9a</td>\n",
       "      <td>\"\\n\\nThe Objectivity of this Discussion is dou...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65432</td>\n",
       "      <td>af0ee0066c607eb8</td>\n",
       "      <td>Shelly Shock\\nShelly Shock is. . .( )</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>154979</td>\n",
       "      <td>b734772b1a807e09</td>\n",
       "      <td>I do not care. Refer to Ong Teng Cheong talk p...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                id  \\\n",
       "0      140030  ed56f082116dcbd0   \n",
       "1      159124  f8e3cd98b63bf401   \n",
       "2       60006  a09e1bcf10631f9a   \n",
       "3       65432  af0ee0066c607eb8   \n",
       "4      154979  b734772b1a807e09   \n",
       "\n",
       "                                        comment_text  toxic  severe_toxic  \\\n",
       "0  Grandma Terri Should Burn in Trash \\nGrandma T...      1             0   \n",
       "1  , 9 May 2009 (UTC)\\nIt would be easiest if you...      0             0   \n",
       "2  \"\\n\\nThe Objectivity of this Discussion is dou...      0             0   \n",
       "3              Shelly Shock\\nShelly Shock is. . .( )      0             0   \n",
       "4  I do not care. Refer to Ong Teng Cheong talk p...      0             0   \n",
       "\n",
       "   obscene  threat  insult  identity_hate  \n",
       "0        0       0       0              0  \n",
       "1        0       0       0              0  \n",
       "2        0       0       0              0  \n",
       "3        0       0       0              0  \n",
       "4        0       0       0              0  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def dataExploration(trainData):   \n",
    "    # look at the top five rows of the train data file\n",
    "    # training data contains a row per comment, with an id, \n",
    "    # the text of the comment, and 6 different labels\n",
    "    trainData.head()\n",
    "    \n",
    "    # exampel of a comment in the train data\n",
    "    print (trainData['comment_text'][0])\n",
    "    print (\"\")\n",
    "    \n",
    "    # the length of the comments varies a lot   \n",
    "    lens = trainData.comment_text.str.len()\n",
    "    print(\"Average length of comments: %d\\nStandard dev: %d\\nLongest Comment: %d\" % (lens.mean(), lens.std(), lens.max()))\n",
    "    \n",
    "    print (\"\")\n",
    "    print (\"# Comments in train data: %d \\n# Comments in test data: %d\" % (len(train),len(test)))\n",
    "    lens.hist()\n",
    "    \n",
    "dataExploration(train)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b8515824-b2dd-4c95-bbf9-dc74c80355db",
    "_uuid": "0151ab55887071aed82d297acb2c6545ed964c2b"
   },
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "c66f79d1-1d9f-4d94-82c1-8026af198f2a",
    "_uuid": "4ba6ef86c82f073bf411785d971a694348c3efa9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>none</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>140030</td>\n",
       "      <td>ed56f082116dcbd0</td>\n",
       "      <td>Grandma Terri Should Burn in Trash \\nGrandma T...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>159124</td>\n",
       "      <td>f8e3cd98b63bf401</td>\n",
       "      <td>, 9 May 2009 (UTC)\\nIt would be easiest if you...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60006</td>\n",
       "      <td>a09e1bcf10631f9a</td>\n",
       "      <td>\"\\n\\nThe Objectivity of this Discussion is dou...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65432</td>\n",
       "      <td>af0ee0066c607eb8</td>\n",
       "      <td>Shelly Shock\\nShelly Shock is. . .( )</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>154979</td>\n",
       "      <td>b734772b1a807e09</td>\n",
       "      <td>I do not care. Refer to Ong Teng Cheong talk p...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                id  \\\n",
       "0      140030  ed56f082116dcbd0   \n",
       "1      159124  f8e3cd98b63bf401   \n",
       "2       60006  a09e1bcf10631f9a   \n",
       "3       65432  af0ee0066c607eb8   \n",
       "4      154979  b734772b1a807e09   \n",
       "\n",
       "                                        comment_text  toxic  severe_toxic  \\\n",
       "0  Grandma Terri Should Burn in Trash \\nGrandma T...      1             0   \n",
       "1  , 9 May 2009 (UTC)\\nIt would be easiest if you...      0             0   \n",
       "2  \"\\n\\nThe Objectivity of this Discussion is dou...      0             0   \n",
       "3              Shelly Shock\\nShelly Shock is. . .( )      0             0   \n",
       "4  I do not care. Refer to Ong Teng Cheong talk p...      0             0   \n",
       "\n",
       "   obscene  threat  insult  identity_hate  none  \n",
       "0        0       0       0              0     0  \n",
       "1        0       0       0              0     1  \n",
       "2        0       0       0              0     1  \n",
       "3        0       0       0              0     1  \n",
       "4        0       0       0              0     1  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COMMENT = 'comment_text'\n",
    "\n",
    "def preProcessData(trainData, testData):\n",
    "    # create a list of all the labels to predict\n",
    "    label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "    \n",
    "    # create a 'none' label so we can see how many comments have no labels\n",
    "    train['none'] = 1-train[label_cols].max(axis=1)\n",
    "    \n",
    "    train.describe()\n",
    "    \n",
    "    # get rid of the empty comments, otherwise sklearn complains\n",
    "    train[COMMENT].fillna(\"unknown\", inplace=True)\n",
    "    test[COMMENT].fillna(\"unknown\", inplace=True)\n",
    "    return label_cols\n",
    "\n",
    "label_cols = preProcessData(train, test)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "480780f1-00c0-4f9a-81e5-fc1932516a80",
    "_uuid": "f2e77e8e6df5e29b620c7a2a0add1438c35af932"
   },
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "b7f11db7-5c12-4eb8-9f2d-0323d629fed9",
    "_uuid": "b043a3fb66c443fab0129e863c134ec813dadb87"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_comments_train: 127656, n_features: 64631\n",
      "n_comments_test: 31915, n_features: 64631\n"
     ]
    }
   ],
   "source": [
    "def tokenize(s): \n",
    "    re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "    return re_tok.sub(r' \\1 ', s).split()\n",
    "\n",
    "# create bag of words representation, as a term document matrix using ngrams\n",
    "def wordRepresentation(trainData, testData):\n",
    "    # TF-IDF gives even better priors than the binarized features. \n",
    "    # it improves leaderboard score from 0.59 to 0.55.\n",
    "    vec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n",
    "                   min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "                   smooth_idf=1, sublinear_tf=1 )\n",
    "\n",
    "    # Extracting features from the training data using a sparse vectorizer\"\n",
    "    train_term_doc = vec.fit_transform(trainData[COMMENT])\n",
    "\n",
    "    # Extracting features from the test data using the same vectorizer\n",
    "    test_term_doc = vec.transform(testData[COMMENT])\n",
    "\n",
    "    # a sparse matrix with only a small number of non-zero elements with the below shape\n",
    "    print(\"n_comments_train: %d, n_features: %d\" % train_term_doc.shape)\n",
    "    print(\"n_comments_test: %d, n_features: %d\" % test_term_doc.shape)\n",
    "    \n",
    "    return train_term_doc, test_term_doc, vec\n",
    "    \n",
    "train_term_doc, test_term_doc, vec = wordRepresentation(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "59131479-a861-4f46-add9-b2af09a51976",
    "_uuid": "5fc487461f4c6fdaea25f2cd471fc801856c6689"
   },
   "source": [
    "## Basic Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "45fc6070-ba13-455b-9274-5c2611e2809c",
    "_uuid": "8b277f01cecd575ed4fcae2e630c0dd8ce979793",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def pr(y_i, y):\n",
    "    p = x[y==y_i].sum(0)\n",
    "    return (p+1) / ((y==y_i).sum()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "2299d24b-5515-4d37-92d9-e7f6b16a290a",
    "_uuid": "926eaa2e40e588f4ef2b86e0a28f8e575c9ed5f4",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "x = train_term_doc\n",
    "test_x = test_term_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "b756c889-a383-4952-9ee9-eca79fd3454f",
    "_uuid": "8652ab2f5f84e77fa395252be9b60be1e44fd583",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# Fit a model for one dependent at a time\n",
    "def get_mdl(y):\n",
    "    y = y.values\n",
    "    r = np.log(pr(1,y) / pr(0,y))\n",
    "    m = LogisticRegression(C=4, dual=True)\n",
    "    x_nb = x.multiply(r)\n",
    "    return m.fit(x_nb, y), r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "_cell_guid": "33fd5f8c-adfc-45a1-9fde-1769a0993e76",
    "_uuid": "0fa103b5406aabdc36ea9ef21612d343e4982fc4"
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('fitting', 'toxic')\n",
      "('fitting', 'severe_toxic')\n",
      "('fitting', 'obscene')\n",
      "('fitting', 'threat')\n",
      "('fitting', 'insult')\n",
      "('fitting', 'identity_hate')\n"
     ]
    }
   ],
   "source": [
    "# fit model and make predictions\n",
    "def getPredictions(test, label_cols):\n",
    "    \n",
    "    model_r_dict = {}\n",
    "    \n",
    "    preds = np.zeros((len(test), len(label_cols)))\n",
    "\n",
    "    for i, j in enumerate(label_cols):\n",
    "        print('fitting', j)\n",
    "        m,r = get_mdl(train[j])\n",
    "        \n",
    "        # store model and r value for future predictions\n",
    "        model_r_dict[i] = r, m \n",
    "        \n",
    "        # get predictions\n",
    "        preds[:,i] = m.predict_proba(test_x.multiply(r))[:,1]\n",
    "\n",
    "    return preds, model_r_dict\n",
    "    \n",
    "preds, model_r_dict = getPredictions(test, label_cols)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save Results to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the submission file and store the results\n",
    "def saveResults(predictions):\n",
    "    submission = pd.DataFrame.from_dict({'id': test['id']})\n",
    "    for idx, col in enumerate(label_cols):\n",
    "        submission[col] = predictions[:,idx]\n",
    "    submission.to_csv('submission_new.csv', index=False)\n",
    "    \n",
    "saveResults(preds)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from sklearn.metrics import log_loss\n",
    "import sys, os, re, csv, codecs, numpy as np, pandas as pd\n",
    "\n",
    "def calc_loss(y_true, y_pred):\n",
    "    return np.mean([log_loss(y_true[:, i], y_pred[:, i]) \n",
    "                    for i in range(y_true.shape[1])])\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import roc_auc_score\n",
    "def auc_score(y_true, y_pred):\n",
    "    return np.mean([roc_auc_score(y_true[:, i], y_pred[:, i]) \n",
    "                    for i in range(y_true.shape[1])])\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Log Loss = ', 0.07104368821910462)\n",
      "('AUC = ', 0.9510252031051949)\n"
     ]
    }
   ],
   "source": [
    "true = pd.read_csv('../dataset/test_new.csv')\n",
    "pred = pd.read_csv('submission_new.csv')\n",
    "\n",
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "y_true = true[list_classes].values\n",
    "y_pred = pred[list_classes].values\n",
    "\n",
    "loss = calc_loss(y_true, y_pred)\n",
    "print (\"Log Loss = \", loss)\n",
    "auc = auc_score(y_true, y_pred)\n",
    "print (\"AUC = \", auc)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a comment: \"F U C !\"\n",
      "\n",
      "     toxic severe_toxic   obscene       threat    insult identity_hate\n",
      "0  0.98773    0.0938766  0.995949  0.000496029  0.780581    0.00598765\n"
     ]
    }
   ],
   "source": [
    "# for demo purposes only\n",
    "def demo(vectorizer, label_cols, model_r_dict):\n",
    "    testing_comment = input(\"Enter a comment: \")\n",
    "\n",
    "    # testing_comment = [\"Yo bitch Ja Rule is more succesful then you'll ever be whats up with you and hating you sad mofuckas...i should bitch slap ur pethedic white faces and get you to kiss my ass you guys sicken me. Ja rule is about pride in da music man. dont diss that shit on him. and nothin is wrong bein like tupac he was a brother too...fuckin white boys get things right next time.\"]\n",
    "\n",
    "    # vectorizer.transform takes list input so pass a list\n",
    "    user_comment = []\n",
    "    user_comment.append(testing_comment)\n",
    "    \n",
    "    # Extracting features from the test data using the vectorizer\n",
    "    test_data_x = vectorizer.transform(user_comment)\n",
    "    \n",
    "    # to store the predictions\n",
    "    prediction = np.zeros((1, len(label_cols)))\n",
    "\n",
    "    # make prediction using the model created\n",
    "    for i, j in enumerate(label_cols):\n",
    "        r = model_r_dict[i][0]\n",
    "        m = model_r_dict[i][1]\n",
    "        prediction[0,i] = m.predict_proba(test_data_x.multiply(r))[0,1]\n",
    "    \n",
    "    # copy the result and display\n",
    "    pred_y = pd.DataFrame(columns=label_cols)\n",
    "    for idx, col in enumerate(label_cols):\n",
    "        pred_y.at[0, col] = prediction[0,idx]  \n",
    "     \n",
    "    print (\"\")\n",
    "    print (pred_y)\n",
    "#     # The String I used above is for the first comment, that is, result of loc 0 in submission file\n",
    "#     print (\"This is the actual result: \", submission.loc[0])\n",
    "    \n",
    "demo(vec, label_cols, model_r_dict)      "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 1
}
