{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import re, string\n",
    "from sklearn.pipeline import make_union\n",
    "from random import randint"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read the train and test data\n",
    "def readInputFiles(train_file_path, test_file_path):\n",
    "    train = pd.read_csv(train_file_path)\n",
    "    test = pd.read_csv(test_file_path)\n",
    "    return train, test\n",
    "    \n",
    "train, test = readInputFiles('../input/train.csv', '../input/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>none</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "      <td>159571.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>0.095844</td>\n",
       "      <td>0.009996</td>\n",
       "      <td>0.052948</td>\n",
       "      <td>0.002996</td>\n",
       "      <td>0.049364</td>\n",
       "      <td>0.008805</td>\n",
       "      <td>0.898321</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>0.294379</td>\n",
       "      <td>0.099477</td>\n",
       "      <td>0.223931</td>\n",
       "      <td>0.054650</td>\n",
       "      <td>0.216627</td>\n",
       "      <td>0.093420</td>\n",
       "      <td>0.302226</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "      <td>1.000000</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "               toxic   severe_toxic        obscene         threat  \\\n",
       "count  159571.000000  159571.000000  159571.000000  159571.000000   \n",
       "mean        0.095844       0.009996       0.052948       0.002996   \n",
       "std         0.294379       0.099477       0.223931       0.054650   \n",
       "min         0.000000       0.000000       0.000000       0.000000   \n",
       "25%         0.000000       0.000000       0.000000       0.000000   \n",
       "50%         0.000000       0.000000       0.000000       0.000000   \n",
       "75%         0.000000       0.000000       0.000000       0.000000   \n",
       "max         1.000000       1.000000       1.000000       1.000000   \n",
       "\n",
       "              insult  identity_hate           none  \n",
       "count  159571.000000  159571.000000  159571.000000  \n",
       "mean        0.049364       0.008805       0.898321  \n",
       "std         0.216627       0.093420       0.302226  \n",
       "min         0.000000       0.000000       0.000000  \n",
       "25%         0.000000       0.000000       1.000000  \n",
       "50%         0.000000       0.000000       1.000000  \n",
       "75%         0.000000       0.000000       1.000000  \n",
       "max         1.000000       1.000000       1.000000  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "def preProcessData(trainData, testData):\n",
    "    # create a list of all the labels to predict\n",
    "    label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "    \n",
    "    # create a 'none' label so we can see how many comments have no labels\n",
    "    train['none'] = 1-train[label_cols].max(axis=1)\n",
    "    \n",
    "    # get rid of the empty comments, otherwise sklearn complains\n",
    "    COMMENT = 'comment_text'\n",
    "    train[COMMENT].fillna(\"unknown\", inplace=True)\n",
    "    test[COMMENT].fillna(\"unknown\", inplace=True)\n",
    "    return label_cols\n",
    "\n",
    "label_cols = preProcessData(train, test)\n",
    "train.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "n_comments_train: 159571, n_features: 432572\n",
      "n_comments_test: 153164, n_features: 432572\n"
     ]
    }
   ],
   "source": [
    "def tokenize(s): \n",
    "    re_tok = re.compile(f'([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "    return re_tok.sub(r' \\1 ', s).split()\n",
    "\n",
    "# create bag of words representation, as a term document matrix using ngrams\n",
    "def wordRepresentation(trainData, testData):\n",
    "    # TF-IDF gives even better priors than the binarized features. \n",
    "    # it improves leaderboard score from 0.59 to 0.55.\n",
    "    \n",
    "    # Vectorizer using word ngram\n",
    "    word_vec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n",
    "               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1, analyzer='word')\n",
    "\n",
    "    # Vectorizer using char ngram \n",
    "    char_vec = TfidfVectorizer(ngram_range=(1,2),\n",
    "                   min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "                   smooth_idf=1, sublinear_tf=1, analyzer='char')\n",
    "\n",
    "    # use both char and word ngrams\n",
    "    vec = make_union(word_vec, char_vec, n_jobs=2)\n",
    "\n",
    "    COMMENT = 'comment_text'\n",
    "        \n",
    "    train_text = trainData[COMMENT]\n",
    "    test_text = testData[COMMENT]\n",
    "\n",
    "    all_text = pd.concat([train_text, test_text])\n",
    "    vec.fit(all_text)\n",
    "    \n",
    "    # Extracting features from the training data using a sparse vectorizer\"\n",
    "    train_term_doc = vec.fit_transform(train_text)\n",
    "    \n",
    "    # Extracting features from the test data using the same vectorizer\n",
    "    test_term_doc = vec.transform(test_text)\n",
    "\n",
    "\n",
    "    # a sparse matrix with only a small number of non-zero elements with the below shape\n",
    "    print(\"n_comments_train: %d, n_features: %d\" % train_term_doc.shape)\n",
    "    print(\"n_comments_test: %d, n_features: %d\" % test_term_doc.shape)\n",
    "    \n",
    "    return train_term_doc, test_term_doc, vec\n",
    "    \n",
    "train_term_doc, test_term_doc, vec = wordRepresentation(train, test)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Basic Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pr(y_i, y):\n",
    "    p = x[y==y_i].sum(0)\n",
    "    return (p+1) / ((y==y_i).sum()+1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "x = train_term_doc\n",
    "test_x = test_term_doc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_mdl(y):\n",
    "    y = y.values\n",
    "    r = np.log(pr(1,y) / pr(0,y))\n",
    "    m = LogisticRegression(C=4, dual=True)\n",
    "    x_nb = x.multiply(r)\n",
    "    return m.fit(x_nb, y), r"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "fitting toxic\n",
      "fitting severe_toxic\n",
      "fitting obscene\n",
      "fitting threat\n",
      "fitting insult\n",
      "fitting identity_hate\n"
     ]
    }
   ],
   "source": [
    "# fit model and make predictions\n",
    "def getPredictions(test, label_cols):\n",
    "    \n",
    "    preds = np.zeros((len(test), len(label_cols)))\n",
    "\n",
    "    for i, j in enumerate(label_cols):\n",
    "        print('fitting', j)\n",
    "        m,r = get_mdl(train[j])\n",
    "        \n",
    "        # get predictions\n",
    "        preds[:,i] = m.predict_proba(test_x.multiply(r))[:,1]\n",
    "\n",
    "    return preds\n",
    "    \n",
    "predictions = getPredictions(test, label_cols)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Save results to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the submission file and store the results\n",
    "def saveResults():\n",
    "    submission = pd.DataFrame.from_dict({'id': test['id']})\n",
    "    for idx, col in enumerate(label_cols):\n",
    "        submission[col] = predictions[:,idx]\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    \n",
    "saveResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
