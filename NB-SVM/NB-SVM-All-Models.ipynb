{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d3b04218-0413-4e6c-8751-5d8a404d73a9",
    "_uuid": "0bca9739b82d5d51e1229243e03ea1b6db35c17e"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This kernel shows how to use NBSVM (Naive Bayes - Support Vector Machine) to create a strong baseline. In this kernel, we use sklearn's logistic regression, rather than SVM, although in practice the two are nearly identical (sklearn uses the liblinear library behind the scenes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "_cell_guid": "ef06cd19-66b6-46bc-bf45-184e12d3f7d4",
    "_uuid": "cca038ca9424a3f66e10262fc9129de807b5f855"
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import re, string\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.pipeline import make_union\n",
    "import sys, os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "from score import calc_auc_score, calc_log_loss\n",
    "import nbsvm\n",
    "from importlib import reload"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a494f561-0c2f-4a38-8973-6b60c22da357",
    "_uuid": "f70ebe669fcf6b434c595cf6fb7a76120bf7809c"
   },
   "source": [
    "# Take Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "_cell_guid": "a494f561-0c2f-4a38-8973-6b60c22da357",
    "_uuid": "f70ebe669fcf6b434c595cf6fb7a76120bf7809c"
   },
   "outputs": [],
   "source": [
    "# read the train and test data\n",
    "def readInputFiles(train_file_path, test_file_path):\n",
    "    train = pd.read_csv(train_file_path)\n",
    "    test = pd.read_csv(test_file_path)\n",
    "    return train, test\n",
    "    \n",
    "train, test = readInputFiles('../dataset/train_new.csv', '../dataset/test_new.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b8515824-b2dd-4c95-bbf9-dc74c80355db",
    "_uuid": "0151ab55887071aed82d297acb2c6545ed964c2b"
   },
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "_cell_guid": "c66f79d1-1d9f-4d94-82c1-8026af198f2a",
    "_uuid": "4ba6ef86c82f073bf411785d971a694348c3efa9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>none</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>140030</td>\n",
       "      <td>ed56f082116dcbd0</td>\n",
       "      <td>Grandma Terri Should Burn in Trash \\nGrandma T...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>159124</td>\n",
       "      <td>f8e3cd98b63bf401</td>\n",
       "      <td>, 9 May 2009 (UTC)\\nIt would be easiest if you...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60006</td>\n",
       "      <td>a09e1bcf10631f9a</td>\n",
       "      <td>\"\\n\\nThe Objectivity of this Discussion is dou...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65432</td>\n",
       "      <td>af0ee0066c607eb8</td>\n",
       "      <td>Shelly Shock\\nShelly Shock is. . .( )</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>154979</td>\n",
       "      <td>b734772b1a807e09</td>\n",
       "      <td>I do not care. Refer to Ong Teng Cheong talk p...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                id  \\\n",
       "0      140030  ed56f082116dcbd0   \n",
       "1      159124  f8e3cd98b63bf401   \n",
       "2       60006  a09e1bcf10631f9a   \n",
       "3       65432  af0ee0066c607eb8   \n",
       "4      154979  b734772b1a807e09   \n",
       "\n",
       "                                        comment_text  toxic  severe_toxic  \\\n",
       "0  Grandma Terri Should Burn in Trash \\nGrandma T...      1             0   \n",
       "1  , 9 May 2009 (UTC)\\nIt would be easiest if you...      0             0   \n",
       "2  \"\\n\\nThe Objectivity of this Discussion is dou...      0             0   \n",
       "3              Shelly Shock\\nShelly Shock is. . .( )      0             0   \n",
       "4  I do not care. Refer to Ong Teng Cheong talk p...      0             0   \n",
       "\n",
       "   obscene  threat  insult  identity_hate  none  \n",
       "0        0       0       0              0     0  \n",
       "1        0       0       0              0     1  \n",
       "2        0       0       0              0     1  \n",
       "3        0       0       0              0     1  \n",
       "4        0       0       0              0     1  "
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COMMENT = 'comment_text'\n",
    "\n",
    "def preProcessData(trainData, testData):\n",
    "    # create a list of all the labels to predict\n",
    "    label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "    \n",
    "    # create a 'none' label so we can see how many comments have no labels\n",
    "    train['none'] = 1-train[label_cols].max(axis=1)\n",
    "    \n",
    "    train.describe()\n",
    "    \n",
    "    # get rid of the empty comments, otherwise sklearn complains\n",
    "    train[COMMENT].fillna(\"unknown\", inplace=True)\n",
    "    test[COMMENT].fillna(\"unknown\", inplace=True)\n",
    "    return label_cols\n",
    "\n",
    "label_cols = preProcessData(train, test)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "480780f1-00c0-4f9a-81e5-fc1932516a80",
    "_uuid": "f2e77e8e6df5e29b620c7a2a0add1438c35af932"
   },
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "b7f11db7-5c12-4eb8-9f2d-0323d629fed9",
    "_uuid": "b043a3fb66c443fab0129e863c134ec813dadb87"
   },
   "outputs": [],
   "source": [
    "def tokenize(s): \n",
    "    re_tok = re.compile('([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "    return re_tok.sub(r' \\1 ', s).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create bag of words representation, as a term document matrix using ngrams\n",
    "def wordRepresentation(trainData, testData, word=True, char=False):\n",
    "    # TF-IDF gives even better priors than the binarized features. \n",
    "    # it improves leaderboard score from 0.59 to 0.55.\n",
    "    \n",
    "     # Vectorizer using word ngram\n",
    "    word_vec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n",
    "               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1, analyzer='word')\n",
    "\n",
    "    # Vectorizer using char ngram \n",
    "    char_vec = TfidfVectorizer(ngram_range=(2,4),\n",
    "                   min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "                   smooth_idf=1, sublinear_tf=1, analyzer='char')\n",
    "\n",
    "    # use both char and word ngrams\n",
    "    if word and char:\n",
    "        vec = make_union(word_vec, char_vec, n_jobs=2)    \n",
    "    elif char:\n",
    "        vec = char_vec\n",
    "    else:\n",
    "        vec = word_vec\n",
    "    \n",
    "    # Extracting features from the training data using a sparse vectorizer\"\n",
    "    train_term_doc = vec.fit_transform(trainData[COMMENT])\n",
    "\n",
    "    # Extracting features from the test data using the same vectorizer\n",
    "    test_term_doc = vec.transform(testData[COMMENT])\n",
    "\n",
    "    return train_term_doc, test_term_doc, vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save results to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# create the submission file and store the results\n",
    "def saveResults(predictions, filename):\n",
    "    submission = pd.DataFrame.from_dict({'id': test['id']})\n",
    "    for idx, col in enumerate(label_cols):\n",
    "        submission[col] = predictions[:,idx]\n",
    "    submission.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Log loss / AUC Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(test, preds, fallback_preds_filename):\n",
    "    try: \n",
    "        true = test\n",
    "    except NameError:\n",
    "        true = pd.read_csv('../dataset/test_new.csv')\n",
    "    try: \n",
    "        y_pred = preds\n",
    "    except NameError:\n",
    "        pred = pd.read_csv(fallback_preds_filename)\n",
    "        y_pred = pred[list_classes].values\n",
    "\n",
    "    y_true = true[list_classes].values\n",
    "\n",
    "    loss = calc_log_loss(y_true, y_pred)\n",
    "    auc = calc_auc_score(y_true, y_pred)\n",
    "    return loss, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict(model_file, train_x, test_x):\n",
    "    # Train the model only if it doesn't exist already\n",
    "    if os.path.isfile(model_file):\n",
    "        print (\"Loading model from saved file \" + model_file)\n",
    "        mdl = joblib.load(model_file)   \n",
    "    else:\n",
    "        print (\"Model doesn't exist. Training mode and saving in file \" + model_file)\n",
    "        mdl = nbsvm.train_model(train, train_x, label_cols, model_file)\n",
    "    preds = nbsvm.get_preds_from_model(mdl, test_x, label_cols)\n",
    "    print (\"Done predicting\")\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Features for Word n-grams, Char n-grams and Word+Char n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_term_doc = [None] * 3\n",
    "test_term_doc = [None] * 3\n",
    "vec = [None] * 3\n",
    "\n",
    "# Get Baseline NB-SVM Model (Word n-grams)\n",
    "train_term_doc[0], test_term_doc[0], vec[0] = wordRepresentation(train, test)\n",
    "\n",
    "# Get NB-SVM Model with Character n-grams\n",
    "train_term_doc[1], test_term_doc[1], vec[1] = wordRepresentation(train, test, word=False, char=True)\n",
    "\n",
    "# Get NB-SVM Model with Character and Word n-grams\n",
    "train_term_doc[2], test_term_doc[2], vec[2] = wordRepresentation(train, test, word=True, char=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "59131479-a861-4f46-add9-b2af09a51976",
    "_uuid": "5fc487461f4c6fdaea25f2cd471fc801856c6689"
   },
   "source": [
    "## Basic Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<module 'nbsvm' from '/scratch/group/puneet-anjali-group/toxic_comment_classification/NB-SVM/nbsvm.py'>"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "reload(nbsvm)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from saved file baseline_nb_svm_v2.pkl\n",
      "Done predicting\n"
     ]
    }
   ],
   "source": [
    "train_x = train_term_doc[0]\n",
    "test_x = test_term_doc[0]\n",
    "model_file = \"baseline_nb_svm_v2.pkl\"\n",
    "results_file = \"submission_baseline_v2.csv\"\n",
    "preds = predict(model_file, train_x, test_x)\n",
    "saveResults(preds, results_file)\n",
    "loss_basic, auc_basic = get_scores(test, preds, fallback_preds_filename=results_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "59131479-a861-4f46-add9-b2af09a51976",
    "_uuid": "5fc487461f4c6fdaea25f2cd471fc801856c6689"
   },
   "source": [
    "## Character n-grams Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_term_doc[1]\n",
    "test_x = test_term_doc[1]\n",
    "model_file = \"baseline_nb_svm_char_ngrams_v2.pkl\"\n",
    "results_file = \"submission_baseline_char_ngrams_v2.csv\"\n",
    "preds = predict(model_file, train_x, test_x)\n",
    "saveResults(preds, results_file)\n",
    "loss_char_ngrams, auc_char_ngrams = get_scores(test, preds, fallback_preds_filename=results_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word + Char n-grams Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_x = train_term_doc[2]\n",
    "test_x = test_term_doc[2]\n",
    "model_file = \"baseline_nb_svm_wordchar_ngrams_v2.pkl\"\n",
    "results_file = \"submission_baseline_wordchar_ngrams_v2.csv\"\n",
    "preds = predict(model_file, train_x, test_x)\n",
    "saveResults(preds, results_file)\n",
    "loss_wordchar_ngrams, auc_wordchar_ngrams = get_scores(test, preds, fallback_preds_filename=results_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Get AUC Scores for All 3 NB Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Baseline\n",
      "Log loss = 0.07103216256195312\n",
      "AUC Score =  0.9510225742700128\n"
     ]
    }
   ],
   "source": [
    "print (\"For Baseline\")\n",
    "print (\"Log loss =\", loss_basic)\n",
    "print (\"AUC Score = \", auc_basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Baseline\n",
      "Log loss = 0.04755385159140398\n",
      "AUC Score =  0.9858201059910116\n"
     ]
    }
   ],
   "source": [
    "print (\"For Baseline\")\n",
    "print (\"Log loss =\", loss_char_ngrams)\n",
    "print (\"AUC Score = \", auc_char_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Baseline\n",
      "Log loss = 0.049262671636504\n",
      "AUC Score =  0.9837744765586365\n"
     ]
    }
   ],
   "source": [
    "print (\"For Baseline\")\n",
    "print (\"Log loss =\", loss_wordchar_ngrams)\n",
    "print (\"AUC Score = \", auc_wordchar_ngrams)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "# for demo purposes only\n",
    "def demo(vectorizer, label_cols, mdl):\n",
    "    testing_comment = input(\"Enter a comment: \")\n",
    "\n",
    "    # testing_comment = [\"Yo bitch Ja Rule is more succesful then you'll ever be whats up with you and hating you sad mofuckas...i should bitch slap ur pethedic white faces and get you to kiss my ass you guys sicken me. Ja rule is about pride in da music man. dont diss that shit on him. and nothin is wrong bein like tupac he was a brother too...fuckin white boys get things right next time.\"]\n",
    "\n",
    "    # vectorizer.transform takes list input so pass a list\n",
    "    user_comment = []\n",
    "    user_comment.append(testing_comment)\n",
    "    \n",
    "    # Extracting features from the test data using the vectorizer\n",
    "    test_data_x = vectorizer.transform(user_comment)\n",
    "    \n",
    "    # to store the predictions\n",
    "    prediction = np.zeros((1, len(label_cols)))\n",
    "\n",
    "    # make prediction using the model created\n",
    "    for i, j in enumerate(label_cols):\n",
    "        r = mdl[i][0]\n",
    "        m = mdl[i][1]\n",
    "        prediction[0,i] = m.predict_proba(test_data_x.multiply(r))[0,1]\n",
    "    \n",
    "    # copy the result and display\n",
    "    pred_y = pd.DataFrame(columns=label_cols)\n",
    "    for idx, col in enumerate(label_cols):\n",
    "        pred_y.at[0, col] = prediction[0,idx]  \n",
    "     \n",
    "    print (\"\")\n",
    "    print (pred_y)\n",
    "#     # The String I used above is for the first comment, that is, result of loc 0 in submission file\n",
    "#     print (\"This is the actual result: \", submission.loc[0])     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Enter a comment: \"Climate change is happening and it's not changing in our favor. If you think differently you're an idiiiot.\"\n",
      "\n",
      "      toxic severe_toxic  obscene       threat    insult identity_hate\n",
      "0  0.216584  0.000358874  0.14275  0.000238601  0.103477    0.00069805\n"
     ]
    }
   ],
   "source": [
    "mdl = joblib.load(\"baseline_nb_svm_wordchar_ngrams_v2.pkl\")\n",
    "demo(vec[2], label_cols, mdl) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
