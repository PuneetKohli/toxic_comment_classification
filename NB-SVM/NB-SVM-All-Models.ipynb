{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d3b04218-0413-4e6c-8751-5d8a404d73a9",
    "_uuid": "0bca9739b82d5d51e1229243e03ea1b6db35c17e"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This kernel shows how to use NBSVM (Naive Bayes - Support Vector Machine) to create a strong baseline. In this kernel, we use sklearn's logistic regression, rather than SVM, although in practice the two are nearly identical (sklearn uses the liblinear library behind the scenes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "_cell_guid": "ef06cd19-66b6-46bc-bf45-184e12d3f7d4",
    "_uuid": "cca038ca9424a3f66e10262fc9129de807b5f855",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import re, string\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.pipeline import make_union\n",
    "import sys, os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "from score import calc_auc_score, calc_log_loss\n",
    "import nbsvm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a494f561-0c2f-4a38-8973-6b60c22da357",
    "_uuid": "f70ebe669fcf6b434c595cf6fb7a76120bf7809c"
   },
   "source": [
    "# Take Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "_cell_guid": "a494f561-0c2f-4a38-8973-6b60c22da357",
    "_uuid": "f70ebe669fcf6b434c595cf6fb7a76120bf7809c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the train and test data\n",
    "def readInputFiles(train_file_path, test_file_path):\n",
    "    train = pd.read_csv(train_file_path)\n",
    "    test = pd.read_csv(test_file_path)\n",
    "    return train, test\n",
    "    \n",
    "train, test = readInputFiles('../dataset/train_new.csv', '../dataset/test_new.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b8515824-b2dd-4c95-bbf9-dc74c80355db",
    "_uuid": "0151ab55887071aed82d297acb2c6545ed964c2b"
   },
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "_cell_guid": "c66f79d1-1d9f-4d94-82c1-8026af198f2a",
    "_uuid": "4ba6ef86c82f073bf411785d971a694348c3efa9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>none</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>140030</td>\n",
       "      <td>ed56f082116dcbd0</td>\n",
       "      <td>Grandma Terri Should Burn in Trash \\nGrandma T...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>159124</td>\n",
       "      <td>f8e3cd98b63bf401</td>\n",
       "      <td>, 9 May 2009 (UTC)\\nIt would be easiest if you...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60006</td>\n",
       "      <td>a09e1bcf10631f9a</td>\n",
       "      <td>\"\\n\\nThe Objectivity of this Discussion is dou...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65432</td>\n",
       "      <td>af0ee0066c607eb8</td>\n",
       "      <td>Shelly Shock\\nShelly Shock is. . .( )</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>154979</td>\n",
       "      <td>b734772b1a807e09</td>\n",
       "      <td>I do not care. Refer to Ong Teng Cheong talk p...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                id  \\\n",
       "0      140030  ed56f082116dcbd0   \n",
       "1      159124  f8e3cd98b63bf401   \n",
       "2       60006  a09e1bcf10631f9a   \n",
       "3       65432  af0ee0066c607eb8   \n",
       "4      154979  b734772b1a807e09   \n",
       "\n",
       "                                        comment_text  toxic  severe_toxic  \\\n",
       "0  Grandma Terri Should Burn in Trash \\nGrandma T...      1             0   \n",
       "1  , 9 May 2009 (UTC)\\nIt would be easiest if you...      0             0   \n",
       "2  \"\\n\\nThe Objectivity of this Discussion is dou...      0             0   \n",
       "3              Shelly Shock\\nShelly Shock is. . .( )      0             0   \n",
       "4  I do not care. Refer to Ong Teng Cheong talk p...      0             0   \n",
       "\n",
       "   obscene  threat  insult  identity_hate  none  \n",
       "0        0       0       0              0     0  \n",
       "1        0       0       0              0     1  \n",
       "2        0       0       0              0     1  \n",
       "3        0       0       0              0     1  \n",
       "4        0       0       0              0     1  "
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COMMENT = 'comment_text'\n",
    "\n",
    "def preProcessData(trainData, testData):\n",
    "    # create a list of all the labels to predict\n",
    "    label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "    \n",
    "    # create a 'none' label so we can see how many comments have no labels\n",
    "    train['none'] = 1-train[label_cols].max(axis=1)\n",
    "    \n",
    "    train.describe()\n",
    "    \n",
    "    # get rid of the empty comments, otherwise sklearn complains\n",
    "    train[COMMENT].fillna(\"unknown\", inplace=True)\n",
    "    test[COMMENT].fillna(\"unknown\", inplace=True)\n",
    "    return label_cols\n",
    "\n",
    "label_cols = preProcessData(train, test)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "480780f1-00c0-4f9a-81e5-fc1932516a80",
    "_uuid": "f2e77e8e6df5e29b620c7a2a0add1438c35af932"
   },
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "_cell_guid": "b7f11db7-5c12-4eb8-9f2d-0323d629fed9",
    "_uuid": "b043a3fb66c443fab0129e863c134ec813dadb87",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def tokenize(s): \n",
    "    re_tok = re.compile('([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "    return re_tok.sub(r' \\1 ', s).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create bag of words representation, as a term document matrix using ngrams\n",
    "def wordRepresentation(trainData, testData, word=True, char=False):\n",
    "    # TF-IDF gives even better priors than the binarized features. \n",
    "    # it improves leaderboard score from 0.59 to 0.55.\n",
    "    \n",
    "     # Vectorizer using word ngram\n",
    "    word_vec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n",
    "               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1, analyzer='word')\n",
    "\n",
    "    # Vectorizer using char ngram \n",
    "    char_vec = TfidfVectorizer(ngram_range=(1,2),\n",
    "                   min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "                   smooth_idf=1, sublinear_tf=1, analyzer='char')\n",
    "\n",
    "    # use both char and word ngrams\n",
    "    if word and char:\n",
    "        vec = make_union(word_vec, char_vec, n_jobs=2)    \n",
    "    elif char:\n",
    "        vec = char_vec\n",
    "    else:\n",
    "        vec = word_vec\n",
    "    \n",
    "    # Extracting features from the training data using a sparse vectorizer\"\n",
    "    train_term_doc = vec.fit_transform(trainData[COMMENT])\n",
    "\n",
    "    # Extracting features from the test data using the same vectorizer\n",
    "    test_term_doc = vec.transform(testData[COMMENT])\n",
    "\n",
    "    return train_term_doc, test_term_doc, vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save results to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the submission file and store the results\n",
    "def saveResults(predictions, filename):\n",
    "    submission = pd.DataFrame.from_dict({'id': test['id']})\n",
    "    for idx, col in enumerate(label_cols):\n",
    "        submission[col] = predictions[:,idx]\n",
    "    submission.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Log loss / AUC Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_scores(test, preds, fallback_preds_filename):\n",
    "    try: \n",
    "        true = test\n",
    "    except NameError:\n",
    "        true = pd.read_csv('../dataset/test_new.csv')\n",
    "    try: \n",
    "        y_pred = preds\n",
    "    except NameError:\n",
    "        pred = pd.read_csv(fallback_preds_filename)\n",
    "        y_pred = pred[list_classes].values\n",
    "\n",
    "    y_true = true[list_classes].values\n",
    "\n",
    "    loss = calc_log_loss(y_true, y_pred)\n",
    "    auc = calc_auc_score(y_true, y_pred)\n",
    "    return loss, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(model_file, train_x, test_x):\n",
    "    # Train the model only if it doesn't exist already\n",
    "    if os.path.isfile(model_file):\n",
    "        print (\"Loading model from saved file \" + model_file)\n",
    "        mdl = joblib.load(model_file)   \n",
    "    else:\n",
    "        print (\"Model doesn't exist. Training mode and saving in file \" + model_file)\n",
    "        mdl = nbsvm.train_model(train, train_x, label_cols, model_file)\n",
    "    preds = nbsvm.get_preds_from_model(mdl, test_x, label_cols)\n",
    "    print (\"Done predicting\")\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Extract Features for Word n-grams, Char n-grams and Word+Char n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "train_term_doc = [None] * 3\n",
    "test_term_doc = [None] * 3\n",
    "vec = [None] * 3\n",
    "\n",
    "# Get Baseline NB-SVM Model (Word n-grams)\n",
    "train_term_doc[0], test_term_doc[0], vec[0] = wordRepresentation(train, test)\n",
    "\n",
    "# Get NB-SVM Model with Character n-grams\n",
    "train_term_doc[1], test_term_doc[1], vec[1] = wordRepresentation(train, test, word=False, char=True)\n",
    "\n",
    "# Get NB-SVM Model with Character and Word n-grams\n",
    "train_term_doc[2], test_term_doc[2], vec[2] = wordRepresentation(train, test, word=True, char=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "59131479-a861-4f46-add9-b2af09a51976",
    "_uuid": "5fc487461f4c6fdaea25f2cd471fc801856c6689"
   },
   "source": [
    "## Basic Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from saved file baseline_nb_svm.pkl\n",
      "Done predicting\n"
     ]
    }
   ],
   "source": [
    "train_x = train_term_doc[0]\n",
    "test_x = test_term_doc[0]\n",
    "model_file = \"baseline_nb_svm.pkl\"\n",
    "results_file = \"submission_baseline.csv\"\n",
    "preds = predict(model_file, train_x, test_x)\n",
    "saveResults(preds, results_file)\n",
    "loss_basic, auc_basic = get_scores(test, preds, fallback_preds_filename=results_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "59131479-a861-4f46-add9-b2af09a51976",
    "_uuid": "5fc487461f4c6fdaea25f2cd471fc801856c6689"
   },
   "source": [
    "## Character n-grams Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from saved file baseline_nb_svm_char_ngrams.pkl\n",
      "Done predicting\n"
     ]
    }
   ],
   "source": [
    "train_x = train_term_doc[1]\n",
    "test_x = test_term_doc[1]\n",
    "model_file = \"baseline_nb_svm_char_ngrams.pkl\"\n",
    "results_file = \"submission_baseline_char_ngrams.csv\"\n",
    "preds = predict(model_file, train_x, test_x)\n",
    "saveResults(preds, results_file)\n",
    "loss_char_ngrams, auc_char_ngrams = get_scores(test, preds, fallback_preds_filename=results_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word + Char n-grams Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from saved file baseline_nb_svm_wordchar_ngrams.pkl\n",
      "Done predicting\n"
     ]
    }
   ],
   "source": [
    "train_x = train_term_doc[2]\n",
    "test_x = test_term_doc[2]\n",
    "model_file = \"baseline_nb_svm_wordchar_ngrams.pkl\"\n",
    "results_file = \"submission_baseline_wordchar_ngrams.csv\"\n",
    "preds = predict(model_file, train_x, test_x)\n",
    "saveResults(preds, results_file)\n",
    "loss_wordchar_ngrams, auc_wordchar_ngrams = get_scores(test, preds, fallback_preds_filename=results_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "collapsed": true
   },
   "source": [
    "# Get AUC Scores for All 3 NB Models"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Baseline\n",
      "('Log loss =', 0.07104368810823293)\n",
      "('AUC Score = ', 0.9510252050191966)\n"
     ]
    }
   ],
   "source": [
    "print (\"For Baseline\")\n",
    "print (\"Log loss =\", loss_basic)\n",
    "print (\"AUC Score = \", auc_basic)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Baseline\n",
      "('Log loss =', 0.07261229052475593)\n",
      "('AUC Score = ', 0.9494655611791685)\n"
     ]
    }
   ],
   "source": [
    "print (\"For Baseline\")\n",
    "print (\"Log loss =\", loss_char_ngrams)\n",
    "print (\"AUC Score = \", auc_char_ngrams)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "For Baseline\n",
      "('Log loss =', 0.06807725729517826)\n",
      "('AUC Score = ', 0.9563577271367983)\n"
     ]
    }
   ],
   "source": [
    "print (\"For Baseline\")\n",
    "print (\"Log loss =\", loss_wordchar_ngrams)\n",
    "print (\"AUC Score = \", auc_wordchar_ngrams)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
