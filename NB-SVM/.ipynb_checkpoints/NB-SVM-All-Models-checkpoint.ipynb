{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "d3b04218-0413-4e6c-8751-5d8a404d73a9",
    "_uuid": "0bca9739b82d5d51e1229243e03ea1b6db35c17e"
   },
   "source": [
    "# Introduction\n",
    "\n",
    "This kernel shows how to use NBSVM (Naive Bayes - Support Vector Machine) to create a strong baseline. In this kernel, we use sklearn's logistic regression, rather than SVM, although in practice the two are nearly identical (sklearn uses the liblinear library behind the scenes)."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 71,
   "metadata": {
    "_cell_guid": "ef06cd19-66b6-46bc-bf45-184e12d3f7d4",
    "_uuid": "cca038ca9424a3f66e10262fc9129de807b5f855"
   },
   "outputs": [],
   "source": [
    "import pandas as pd, numpy as np\n",
    "from sklearn.linear_model import LogisticRegression\n",
    "from sklearn.feature_extraction.text import CountVectorizer, TfidfVectorizer\n",
    "import re, string\n",
    "from sklearn.externals import joblib\n",
    "from sklearn.pipeline import make_union\n",
    "import sys, os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "from score import calc_auc_score, calc_log_loss\n",
    "import nbsvm"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "a494f561-0c2f-4a38-8973-6b60c22da357",
    "_uuid": "f70ebe669fcf6b434c595cf6fb7a76120bf7809c"
   },
   "source": [
    "# Take Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "_cell_guid": "a494f561-0c2f-4a38-8973-6b60c22da357",
    "_uuid": "f70ebe669fcf6b434c595cf6fb7a76120bf7809c",
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read the train and test data\n",
    "def readInputFiles(train_file_path, test_file_path):\n",
    "    train = pd.read_csv(train_file_path)\n",
    "    test = pd.read_csv(test_file_path)\n",
    "    return train, test\n",
    "    \n",
    "train, test = readInputFiles('../dataset/train_new.csv', '../dataset/test_new.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "b8515824-b2dd-4c95-bbf9-dc74c80355db",
    "_uuid": "0151ab55887071aed82d297acb2c6545ed964c2b"
   },
   "source": [
    "# Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "_cell_guid": "c66f79d1-1d9f-4d94-82c1-8026af198f2a",
    "_uuid": "4ba6ef86c82f073bf411785d971a694348c3efa9"
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Unnamed: 0</th>\n",
       "      <th>id</th>\n",
       "      <th>comment_text</th>\n",
       "      <th>toxic</th>\n",
       "      <th>severe_toxic</th>\n",
       "      <th>obscene</th>\n",
       "      <th>threat</th>\n",
       "      <th>insult</th>\n",
       "      <th>identity_hate</th>\n",
       "      <th>none</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>140030</td>\n",
       "      <td>ed56f082116dcbd0</td>\n",
       "      <td>Grandma Terri Should Burn in Trash \\nGrandma T...</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>159124</td>\n",
       "      <td>f8e3cd98b63bf401</td>\n",
       "      <td>, 9 May 2009 (UTC)\\nIt would be easiest if you...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>60006</td>\n",
       "      <td>a09e1bcf10631f9a</td>\n",
       "      <td>\"\\n\\nThe Objectivity of this Discussion is dou...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>65432</td>\n",
       "      <td>af0ee0066c607eb8</td>\n",
       "      <td>Shelly Shock\\nShelly Shock is. . .( )</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>154979</td>\n",
       "      <td>b734772b1a807e09</td>\n",
       "      <td>I do not care. Refer to Ong Teng Cheong talk p...</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   Unnamed: 0                id  \\\n",
       "0      140030  ed56f082116dcbd0   \n",
       "1      159124  f8e3cd98b63bf401   \n",
       "2       60006  a09e1bcf10631f9a   \n",
       "3       65432  af0ee0066c607eb8   \n",
       "4      154979  b734772b1a807e09   \n",
       "\n",
       "                                        comment_text  toxic  severe_toxic  \\\n",
       "0  Grandma Terri Should Burn in Trash \\nGrandma T...      1             0   \n",
       "1  , 9 May 2009 (UTC)\\nIt would be easiest if you...      0             0   \n",
       "2  \"\\n\\nThe Objectivity of this Discussion is dou...      0             0   \n",
       "3              Shelly Shock\\nShelly Shock is. . .( )      0             0   \n",
       "4  I do not care. Refer to Ong Teng Cheong talk p...      0             0   \n",
       "\n",
       "   obscene  threat  insult  identity_hate  none  \n",
       "0        0       0       0              0     0  \n",
       "1        0       0       0              0     1  \n",
       "2        0       0       0              0     1  \n",
       "3        0       0       0              0     1  \n",
       "4        0       0       0              0     1  "
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "COMMENT = 'comment_text'\n",
    "\n",
    "def preProcessData(trainData, testData):\n",
    "    # create a list of all the labels to predict\n",
    "    label_cols = ['toxic', 'severe_toxic', 'obscene', 'threat', 'insult', 'identity_hate']\n",
    "    \n",
    "    # create a 'none' label so we can see how many comments have no labels\n",
    "    train['none'] = 1-train[label_cols].max(axis=1)\n",
    "    \n",
    "    train.describe()\n",
    "    \n",
    "    # get rid of the empty comments, otherwise sklearn complains\n",
    "    train[COMMENT].fillna(\"unknown\", inplace=True)\n",
    "    test[COMMENT].fillna(\"unknown\", inplace=True)\n",
    "    return label_cols\n",
    "\n",
    "label_cols = preProcessData(train, test)\n",
    "train.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "480780f1-00c0-4f9a-81e5-fc1932516a80",
    "_uuid": "f2e77e8e6df5e29b620c7a2a0add1438c35af932"
   },
   "source": [
    "# Feature Extraction"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "_cell_guid": "b7f11db7-5c12-4eb8-9f2d-0323d629fed9",
    "_uuid": "b043a3fb66c443fab0129e863c134ec813dadb87"
   },
   "outputs": [],
   "source": [
    "def tokenize(s): \n",
    "    re_tok = re.compile('([{string.punctuation}“”¨«»®´·º½¾¿¡§£₤‘’])')\n",
    "    return re_tok.sub(r' \\1 ', s).split()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create bag of words representation, as a term document matrix using ngrams\n",
    "def wordRepresentation(trainData, testData, word=True, char=False):\n",
    "    # TF-IDF gives even better priors than the binarized features. \n",
    "    # it improves leaderboard score from 0.59 to 0.55.\n",
    "    \n",
    "     # Vectorizer using word ngram\n",
    "    word_vec = TfidfVectorizer(ngram_range=(1,2), tokenizer=tokenize,\n",
    "               min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "               smooth_idf=1, sublinear_tf=1, analyzer='word')\n",
    "\n",
    "    # Vectorizer using char ngram \n",
    "    char_vec = TfidfVectorizer(ngram_range=(1,2),\n",
    "                   min_df=3, max_df=0.9, strip_accents='unicode', use_idf=1,\n",
    "                   smooth_idf=1, sublinear_tf=1, analyzer='char')\n",
    "\n",
    "    # use both char and word ngrams\n",
    "    if word and char:\n",
    "        vec = make_union(word_vec, char_vec, n_jobs=2)    \n",
    "    elif char:\n",
    "        vec = char_vec\n",
    "    else:\n",
    "        vec = word_vec\n",
    "    \n",
    "    # Extracting features from the training data using a sparse vectorizer\"\n",
    "    train_term_doc = vec.fit_transform(trainData[COMMENT])\n",
    "\n",
    "    # Extracting features from the test data using the same vectorizer\n",
    "    test_term_doc = vec.transform(testData[COMMENT])\n",
    "\n",
    "    return train_term_doc, test_term_doc, vec"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Save results to a file"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 55,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the submission file and store the results\n",
    "def saveResults(predictions, filename):\n",
    "    submission = pd.DataFrame.from_dict({'id': test['id']})\n",
    "    for idx, col in enumerate(label_cols):\n",
    "        submission[col] = predictions[:,idx]\n",
    "    submission.to_csv(filename, index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Calculate Log loss / AUC Score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_scores(test, preds, fallback_preds_filename):\n",
    "    try: \n",
    "        true = test\n",
    "    except NameError:\n",
    "        true = pd.read_csv('../dataset/test_new.csv')\n",
    "    try: \n",
    "        y_pred = preds\n",
    "    except NameError:\n",
    "        pred = pd.read_csv(fallback_preds_filename)\n",
    "        y_pred = pred[list_classes].values\n",
    "\n",
    "    y_true = true[list_classes].values\n",
    "\n",
    "    loss = calc_log_loss(y_true, y_pred)\n",
    "    auc = calc_auc_score(y_true, y_pred)\n",
    "    return loss, auc"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Extract Features for Word n-grams, Char n-grams and Word+Char n-grams"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_term_doc = [None] * 3\n",
    "test_term_doc = [None] * 3\n",
    "vec = [None] * 3\n",
    "\n",
    "# Get Baseline NB-SVM Model (Word n-grams)\n",
    "train_term_doc[0], test_term_doc[0], vec[0] = wordRepresentation(train, test)\n",
    "\n",
    "# Get NB-SVM Model with Character n-grams\n",
    "train_term_doc[1], test_term_doc[1], vec[1] = wordRepresentation(train, test, word=False, char=True)\n",
    "\n",
    "# Get NB-SVM Model with Character and Word n-grams\n",
    "train_term_doc[2], test_term_doc[2], vec[2] = wordRepresentation(train, test, word=True, char=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Predict Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 73,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def predict(model_file, train_x, test_x):\n",
    "    # Train the model only if it doesn't exist already\n",
    "    if os.path.isfile(model_file):\n",
    "        print (\"Loading model from saved file \" + model_file)\n",
    "        mdl = joblib.load(model_file)   \n",
    "    else:\n",
    "        print (\"Model doesn't exist. Training mode and saving in file \" + model_file)\n",
    "        mdl = nbsvm.train_model(train, train_x, label_cols, model_file)\n",
    "    preds = nbsvm.get_preds_from_model(mdl, test_x, label_cols)\n",
    "    print (\"Done predicting\")\n",
    "    return preds"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "59131479-a861-4f46-add9-b2af09a51976",
    "_uuid": "5fc487461f4c6fdaea25f2cd471fc801856c6689"
   },
   "source": [
    "## Basic Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 74,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from saved file baseline_nb_svm.pkl\n"
     ]
    },
    {
     "ename": "AttributeError",
     "evalue": "'str' object has no attribute 'readline'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mAttributeError\u001b[0m                            Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-74-4d24b36d23b2>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"baseline_nb_svm.pkl\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mresults_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"submission_baseline.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0msaveResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mloss_basic\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc_basic\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfallback_preds_filename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresults_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-73-e4b9196c544e>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(model_file, train_x, test_x)\u001b[0m\n\u001b[1;32m      4\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0misfile\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      5\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading model from saved file \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 6\u001b[0;31m         \u001b[0mmdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpickle\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      7\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      8\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Model doesn't exist. Training mode and saving in file \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python@2/2.7.14_3/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(file)\u001b[0m\n\u001b[1;32m   1382\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1383\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m-> 1384\u001b[0;31m     \u001b[0;32mreturn\u001b[0m \u001b[0mUnpickler\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfile\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m   1385\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m   1386\u001b[0m \u001b[0;32mdef\u001b[0m \u001b[0mloads\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mstr\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/Cellar/python@2/2.7.14_3/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36m__init__\u001b[0;34m(self, file)\u001b[0m\n\u001b[1;32m    845\u001b[0m         \u001b[0;32mor\u001b[0m \u001b[0many\u001b[0m \u001b[0mother\u001b[0m \u001b[0mcustom\u001b[0m \u001b[0mobject\u001b[0m \u001b[0mthat\u001b[0m \u001b[0mmeets\u001b[0m \u001b[0mthis\u001b[0m \u001b[0minterface\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    846\u001b[0m         \"\"\"\n\u001b[0;32m--> 847\u001b[0;31m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mreadline\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    848\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mfile\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mread\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    849\u001b[0m         \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mmemo\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m{\u001b[0m\u001b[0;34m}\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mAttributeError\u001b[0m: 'str' object has no attribute 'readline'"
     ]
    }
   ],
   "source": [
    "train_x = train_term_doc[0]\n",
    "test_x = test_term_doc[0]\n",
    "model_file = \"baseline_nb_svm.pkl\"\n",
    "results_file = \"submission_baseline.csv\"\n",
    "preds = predict(model_file, train_x, test_x)\n",
    "saveResults(preds, results_file)\n",
    "loss_basic, auc_basic = get_scores(test, preds, fallback_preds_filename=results_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "_cell_guid": "59131479-a861-4f46-add9-b2af09a51976",
    "_uuid": "5fc487461f4c6fdaea25f2cd471fc801856c6689"
   },
   "source": [
    "## Character n-grams Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from saved file baseline_nb_svm_char_ngrams.pkl\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'\\xd3'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-63-e2ee913831ee>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"baseline_nb_svm_char_ngrams.pkl\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mresults_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"submission_baseline_char_ngrams.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0msaveResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mloss_char_ngrams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc_char_ngrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfallback_preds_filename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresults_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-7512c8bf0152>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(model_file, train_x, test_x)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading model from saved file \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mmdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Model doesn't exist. Training mode and saving in file \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/externals/joblib/numpy_pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    576\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mload_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/externals/joblib/numpy_pickle.pyc\u001b[0m in \u001b[0;36m_unpickle\u001b[0;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             warnings.warn(\"The file '%s' has been generated with a \"\n",
      "\u001b[0;32m/usr/local/Cellar/python@2/2.7.14_3/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 864\u001b[0;31m                 \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '\\xd3'"
     ]
    }
   ],
   "source": [
    "train_x = train_term_doc[1]\n",
    "test_x = test_term_doc[1]\n",
    "model_file = \"baseline_nb_svm_char_ngrams.pkl\"\n",
    "results_file = \"submission_baseline_char_ngrams.csv\"\n",
    "preds = predict(model_file, train_x, test_x)\n",
    "saveResults(preds, results_file)\n",
    "loss_char_ngrams, auc_char_ngrams = get_scores(test, preds, fallback_preds_filename=results_file)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Word + Char n-grams Naive Bayes Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 67,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading model from saved file baseline_nb_svm_wordchar_ngrams.pkl\n"
     ]
    },
    {
     "ename": "KeyError",
     "evalue": "'\\xbf'",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyError\u001b[0m                                  Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-67-b37201052c1b>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[1;32m      3\u001b[0m \u001b[0mmodel_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"baseline_nb_svm_wordchar_ngrams.pkl\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m \u001b[0mresults_file\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m\"submission_baseline_wordchar_ngrams.csv\"\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m \u001b[0mpreds\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpredict\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_x\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtest_x\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m \u001b[0msaveResults\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mresults_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m \u001b[0mloss_wordchar_ngrams\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mauc_wordchar_ngrams\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mget_scores\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtest\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mpreds\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfallback_preds_filename\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mresults_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m<ipython-input-57-7512c8bf0152>\u001b[0m in \u001b[0;36mpredict\u001b[0;34m(model_file, train_x, test_x)\u001b[0m\n\u001b[1;32m      3\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mos\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mpath\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mexists\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      4\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Loading model from saved file \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m----> 5\u001b[0;31m         \u001b[0mmdl\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mjoblib\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m      6\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m      7\u001b[0m         \u001b[0;32mprint\u001b[0m \u001b[0;34m(\u001b[0m\u001b[0;34m\"Model doesn't exist. Training mode and saving in file \"\u001b[0m \u001b[0;34m+\u001b[0m \u001b[0mmodel_file\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/externals/joblib/numpy_pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(filename, mmap_mode)\u001b[0m\n\u001b[1;32m    576\u001b[0m                     \u001b[0;32mreturn\u001b[0m \u001b[0mload_compatibility\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    577\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 578\u001b[0;31m                 \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0m_unpickle\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mfobj\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mfilename\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmmap_mode\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    579\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    580\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mobj\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/sklearn/externals/joblib/numpy_pickle.pyc\u001b[0m in \u001b[0;36m_unpickle\u001b[0;34m(fobj, filename, mmap_mode)\u001b[0m\n\u001b[1;32m    506\u001b[0m     \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mNone\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    507\u001b[0m     \u001b[0;32mtry\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 508\u001b[0;31m         \u001b[0mobj\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mload\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    509\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0munpickler\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mcompat_mode\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    510\u001b[0m             warnings.warn(\"The file '%s' has been generated with a \"\n",
      "\u001b[0;32m/usr/local/Cellar/python@2/2.7.14_3/Frameworks/Python.framework/Versions/2.7/lib/python2.7/pickle.pyc\u001b[0m in \u001b[0;36mload\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    862\u001b[0m             \u001b[0;32mwhile\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    863\u001b[0m                 \u001b[0mkey\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mread\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;36m1\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 864\u001b[0;31m                 \u001b[0mdispatch\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0mkey\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mself\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    865\u001b[0m         \u001b[0;32mexcept\u001b[0m \u001b[0m_Stop\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    866\u001b[0m             \u001b[0;32mreturn\u001b[0m \u001b[0mstopinst\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mvalue\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyError\u001b[0m: '\\xbf'"
     ]
    }
   ],
   "source": [
    "train_x = train_term_doc[2]\n",
    "test_x = test_term_doc[2]\n",
    "model_file = \"baseline_nb_svm_wordchar_ngrams.pkl\"\n",
    "results_file = \"submission_baseline_wordchar_ngrams.csv\"\n",
    "preds = predict(model_file, train_x, test_x)\n",
    "saveResults(preds, results_file)\n",
    "loss_wordchar_ngrams, auc_wordchar_ngrams = get_scores(test, preds, fallback_preds_filename=results_file)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
