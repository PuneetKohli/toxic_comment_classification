{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/skgill/anaconda2/lib/python2.7/site-packages/h5py/__init__.py:34: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from subprocess import check_output"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# read files\n",
    "def readInputFiles(train_file_path, test_file_path):\n",
    "    train = pd.read_csv(train_file_path)\n",
    "    test = pd.read_csv(test_file_path)\n",
    "    train = train.sample(frac=1)\n",
    "    return train, test\n",
    "    \n",
    "train, test = readInputFiles('../input/train.csv', '../input/test.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "def preProcessData():\n",
    "    max_features = 20000\n",
    "    maxlen = 100\n",
    "\n",
    "    # grab all the comments from train and fill the NAN comments with CVxTz\n",
    "    list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").values\n",
    "\n",
    "    # get the values for 6 classes\n",
    "    list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "    y = train[list_classes].values\n",
    "\n",
    "    # grab all the comments from test and fill the NAN comments with CVxTz\n",
    "    list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").values\n",
    "\n",
    "    tokenizer = text.Tokenizer(num_words=max_features)\n",
    "\n",
    "    # only use the training data comments for tokenizer\n",
    "    tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "\n",
    "    # convert form strings to list of indices of words\n",
    "    list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "    list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "\n",
    "    # truncate list if length over 100\n",
    "    # pad list if length less than 100\n",
    "    X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "    X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "    \n",
    "    return max_features, maxlen, X_train, X_test, y\n",
    "    \n",
    "max_features, maxlen, X_train, X_test, y = preProcessData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "# configure a model\n",
    "def get_model():\n",
    "    embed_size = 128\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "    x = Bidirectional(LSTM(50, return_sequences=True))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(50, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model\n",
    "\n",
    "print \"done\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Train on 143613 samples, validate on 15958 samples\n",
      "Epoch 1/2\n",
      "143613/143613 [==============================] - 1511s 11ms/step - loss: 0.0624 - acc: 0.9790 - val_loss: 0.0497 - val_acc: 0.9817\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04973, saving model to weights_base.best.hdf5\n",
      "Epoch 2/2\n",
      "143613/143613 [==============================] - 1569s 11ms/step - loss: 0.0458 - acc: 0.9832 - val_loss: 0.0481 - val_acc: 0.9820\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04973 to 0.04812, saving model to weights_base.best.hdf5\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "# create a model\n",
    "\n",
    "def createModel(file_path):\n",
    "    model = get_model()\n",
    "    batch_size = 32\n",
    "    epochs = 2\n",
    "\n",
    "    checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "    # The point is to let the script run for more epochs locally \n",
    "    # because it will timeout if done so in Kaggle Kernels\n",
    "    early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
    "\n",
    "    callbacks_list = [checkpoint, early] #early\n",
    "    model.fit(X_train, y, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=callbacks_list)\n",
    "\n",
    "    model.load_weights(file_path)\n",
    "\n",
    "    return model\n",
    "print \"done\"\n",
    "    \n",
    "# file_path: file where model gets stored\n",
    "model = createModel(\"weights_base.best.hdf5\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "# get predictions\n",
    "predictions = model.predict(X_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# create the submission file and store the results\n",
    "def saveResults():\n",
    "    submission = pd.DataFrame.from_dict({'id': test['id']})\n",
    "    for idx, col in enumerate(list_classes):\n",
    "        submission[col] = predictions[:,idx]\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    \n",
    "saveResults()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
