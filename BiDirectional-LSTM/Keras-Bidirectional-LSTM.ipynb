{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np \n",
    "import pandas as pd \n",
    "from keras.models import Model\n",
    "from keras.layers import Dense, Embedding, Input\n",
    "from keras.layers import LSTM, Bidirectional, GlobalMaxPool1D, Dropout\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import EarlyStopping, ModelCheckpoint\n",
    "from subprocess import check_output\n",
    "\n",
    "import sys, os\n",
    "sys.path.append(os.path.dirname(os.getcwd()))\n",
    "from score import calc_auc_score, calc_log_loss"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Take Input"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read files\n",
    "def readInputFiles(train_file_path, test_file_path):\n",
    "    train = pd.read_csv(train_file_path)\n",
    "    test = pd.read_csv(test_file_path)\n",
    "    train = train.sample(frac=1)\n",
    "    return train, test\n",
    "    \n",
    "train, test = readInputFiles('../dataset/train_new.csv', '../dataset/test_new.csv')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preprocessing Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "def preProcessData():\n",
    "    max_features = 20000\n",
    "    maxlen = 200\n",
    "\n",
    "    # grab all the comments from train and fill the NAN comments with CVxTz\n",
    "    list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").values\n",
    "    # grab all the comments from test and fill the NAN comments with CVxTz\n",
    "    list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").values\n",
    "\n",
    "    # get the values for 6 classes\n",
    "    y = train[list_classes].values\n",
    "\n",
    "    tokenizer = text.Tokenizer(num_words=max_features)\n",
    "\n",
    "    # only use the training data comments for tokenizer\n",
    "    tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "\n",
    "    # convert form strings to list of indices of words\n",
    "    list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "    list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "\n",
    "    # truncate list if length over 100\n",
    "    # pad list if length less than 100\n",
    "    X_train = sequence.pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "    X_test = sequence.pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "    \n",
    "    return max_features, maxlen, X_train, X_test, y\n",
    "    \n",
    "max_features, maxlen, X_train, X_test, y = preProcessData()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    embed_size = 128\n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "    x = Bidirectional(LSTM(50, return_sequences=True))(x)\n",
    "    x = GlobalMaxPool1D()(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(50, activation=\"relu\")(x)\n",
    "    x = Dropout(0.1)(x)\n",
    "    x = Dense(6, activation=\"sigmoid\")(x)\n",
    "    model = Model(inputs=inp, outputs=x)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def createModel(file_path):\n",
    "    model = get_model()\n",
    "    batch_size = 32\n",
    "    epochs = 2\n",
    "\n",
    "    checkpoint = ModelCheckpoint(file_path, monitor='val_loss', verbose=1, save_best_only=True, mode='min')\n",
    "\n",
    "    # The point is to let the script run for more epochs locally \n",
    "    # because it will timeout if done so in Kaggle Kernels\n",
    "    early = EarlyStopping(monitor=\"val_loss\", mode=\"min\", patience=20)\n",
    "\n",
    "    callbacks_list = [checkpoint, early] #early\n",
    "    model.fit(X_train, y, batch_size=batch_size, epochs=epochs, validation_split=0.1, callbacks=callbacks_list)\n",
    "\n",
    "    model.load_weights(file_path)\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model if doesn't exist already "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start fitting...\n",
      "(\"Model doesn't exist already, training model and saving at path \", 'bidirectional_lstm_2.h5')\n",
      "Train on 114890 samples, validate on 12766 samples\n",
      "Epoch 1/2\n",
      "114890/114890 [==============================] - 39188s 341ms/step - loss: 0.0686 - acc: 0.9776 - val_loss: 0.0461 - val_acc: 0.9829\n",
      "\n",
      "Epoch 00001: val_loss improved from inf to 0.04612, saving model to weights_base.best.hdf5\n",
      "Epoch 2/2\n",
      "114890/114890 [==============================] - 1273s 11ms/step - loss: 0.0457 - acc: 0.9830 - val_loss: 0.0448 - val_acc: 0.9832\n",
      "\n",
      "Epoch 00002: val_loss improved from 0.04612 to 0.04480, saving model to weights_base.best.hdf5\n"
     ]
    }
   ],
   "source": [
    "print(\"start fitting...\")\n",
    "file_path = \"bidirectional_lstm_2.h5\"\n",
    "if os.path.isfile(file_path):\n",
    "    print (\"Model already exists. Loading from path \", file_path)\n",
    "    model = load_model(file_path)\n",
    "else:\n",
    "    print (\"Model doesn't exist already, training model and saving at path \", file_path)\n",
    "    model = createModel(\"weights_base.best.hdf5\")\n",
    "    model.save(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start prediccting...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"start prediccting...\")\n",
    "y_pred = model.predict(X_test, batch_size=1024)\n",
    "print (\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "\n",
    "# create the submission file and store the results\n",
    "def saveResults():\n",
    "    submission = pd.DataFrame.from_dict({'id': test['id']})\n",
    "    for idx, col in enumerate(list_classes):\n",
    "        submission[col] = y_pred[:,idx]\n",
    "    submission.to_csv('submission.csv', index=False)\n",
    "    \n",
    "saveResults()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_scores(test, preds, fallback_preds_filename):\n",
    "    try: \n",
    "        true = test\n",
    "    except NameError:\n",
    "        true = pd.read_csv('../dataset/test_new.csv')\n",
    "    try: \n",
    "        y_pred = preds\n",
    "    except NameError:\n",
    "        pred = pd.read_csv(fallback_preds_filename)\n",
    "        y_pred = pred[list_classes].values\n",
    "\n",
    "    y_true = true[list_classes].values\n",
    "\n",
    "    loss = calc_log_loss(y_true, y_pred)\n",
    "    auc = calc_auc_score(y_true, y_pred)\n",
    "    return loss, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "true = pd.read_csv('../dataset/test_new.csv')\n",
    "pred = y_pred\n",
    "\n",
    "loss_, aucs = get_scores(true, y_pred, fallback_preds_filename=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "('Log loss = ', 0.046999667351311114)\n",
      "('AUC Score = ', 0.9789796280620561)\n"
     ]
    }
   ],
   "source": [
    "print (\"Log loss = \", loss_)\n",
    "print (\"AUC Score = \", aucs)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 2",
   "language": "python",
   "name": "python2"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.14"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
