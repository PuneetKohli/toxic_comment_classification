{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/usr/local/lib/python2.7/site-packages/h5py/__init__.py:36: FutureWarning: Conversion of the second argument of issubdtype from `float` to `np.floating` is deprecated. In future, it will be treated as `np.float64 == np.dtype(float).type`.\n",
      "  from ._conv import register_converters as _register_converters\n",
      "Using TensorFlow backend.\n"
     ]
    }
   ],
   "source": [
    "import numpy as np\n",
    "np.random.seed(42)\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import roc_auc_score\n",
    "\n",
    "from keras.preprocessing.text import Tokenizer\n",
    "from keras.preprocessing.sequence import pad_sequences\n",
    "from keras.models import Model\n",
    "from keras.models import load_model\n",
    "from keras.layers import Input, Dense, Embedding, SpatialDropout1D, concatenate\n",
    "from keras.layers import GRU, Bidirectional, GlobalAveragePooling1D, GlobalMaxPooling1D\n",
    "from keras.preprocessing import text, sequence\n",
    "from keras.callbacks import Callback"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'UTF-8'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import IPython\n",
    "IPython.utils.encoding.getdefaultencoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'ascii'"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "import sys\n",
    "sys.getdefaultencoding()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import warnings\n",
    "warnings.filterwarnings('ignore')\n",
    "\n",
    "import os\n",
    "os.environ['OMP_NUM_THREADS'] = '4'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "# read files\n",
    "def readInputFiles(train_file_path, test_file_path):\n",
    "    train = pd.read_csv(train_file_path)\n",
    "    test = pd.read_csv(test_file_path)\n",
    "    train = train.sample(frac=1)\n",
    "    return train, test\n",
    "    \n",
    "train, test = readInputFiles('../dataset/train_new.csv', '../dataset/test_new.csv')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "EMBEDDING_FILE = '../embeddings/fasttext/crawl-300d-2M.vec'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "list_classes = [\"toxic\", \"severe_toxic\", \"obscene\", \"threat\", \"insult\", \"identity_hate\"]\n",
    "max_features = 20000\n",
    "maxlen = 200\n",
    "embed_size = 300\n",
    "\n",
    "def preprocess_data():\n",
    "    \n",
    "    y = train[list_classes].values\n",
    "    y_test = test[list_classes].values\n",
    "    list_sentences_train = train[\"comment_text\"].fillna(\"CVxTz\").values\n",
    "    list_sentences_test = test[\"comment_text\"].fillna(\"CVxTz\").values\n",
    "   \n",
    "    tokenizer = Tokenizer(num_words=max_features)\n",
    "\n",
    "    tokenizer.fit_on_texts(list(list_sentences_train))\n",
    "    \n",
    "    list_tokenized_train = tokenizer.texts_to_sequences(list_sentences_train)\n",
    "    list_tokenized_test = tokenizer.texts_to_sequences(list_sentences_test)\n",
    "    \n",
    "    X_train = pad_sequences(list_tokenized_train, maxlen=maxlen)\n",
    "    X_test = pad_sequences(list_tokenized_test, maxlen=maxlen)\n",
    "    \n",
    "    return max_features, maxlen, X_train, X_test, y, y_test, tokenizer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-9-595d2313ef6c>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m()\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mmax_features\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mmaxlen\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_train\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mX_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my_test\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mpreprocess_data\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m<ipython-input-7-45b3be44c64b>\u001b[0m in \u001b[0;36mpreprocess_data\u001b[0;34m()\u001b[0m\n\u001b[1;32m     13\u001b[0m     \u001b[0mtokenizer\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mTokenizer\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnum_words\u001b[0m\u001b[0;34m=\u001b[0m\u001b[0mmax_features\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     14\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 15\u001b[0;31m     \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit_on_texts\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_sentences_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     16\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     17\u001b[0m     \u001b[0mlist_tokenized_train\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtokenizer\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtexts_to_sequences\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlist_sentences_train\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/keras/preprocessing/text.pyc\u001b[0m in \u001b[0;36mfit_on_texts\u001b[0;34m(self, texts)\u001b[0m\n\u001b[1;32m    207\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfilters\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    208\u001b[0m                                             \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mlower\u001b[0m\u001b[0;34m,\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 209\u001b[0;31m                                             self.split)\n\u001b[0m\u001b[1;32m    210\u001b[0m             \u001b[0;32mfor\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    211\u001b[0m                 \u001b[0;32mif\u001b[0m \u001b[0mw\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mself\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mword_counts\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/usr/local/lib/python2.7/site-packages/keras/preprocessing/text.pyc\u001b[0m in \u001b[0;36mtext_to_word_sequence\u001b[0;34m(text, filters, lower, split)\u001b[0m\n\u001b[1;32m     56\u001b[0m         \u001b[0mtext\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mtranslate\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mtranslate_map\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     57\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m---> 58\u001b[0;31m     \u001b[0mseq\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mtext\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0msplit\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m     59\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0mi\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mi\u001b[0m \u001b[0;32min\u001b[0m \u001b[0mseq\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mi\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m     60\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "max_features, maxlen, X_train, X_test, y, y_test, tokenizer = preprocess_data()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_coefs(word, *arr): return word, np.asarray(arr, dtype='float32')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "embeddings_index = dict(get_coefs(*o.rstrip().rsplit(' ')) for o in open(EMBEDDING_FILE, encoding='utf-8'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "word_index = tokenizer.word_index\n",
    "nb_words = min(max_features, len(word_index))\n",
    "embedding_matrix = np.zeros((nb_words, embed_size))\n",
    "for word, i in word_index.items():\n",
    "    if i >= max_features: continue\n",
    "    embedding_vector = embeddings_index.get(word)\n",
    "    if embedding_vector is not None: embedding_matrix[i] = embedding_vector\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_model():\n",
    "    \n",
    "    inp = Input(shape=(maxlen, ))\n",
    "    x = Embedding(max_features, embed_size)(inp)\n",
    "    x = SpatialDropout1D(0.2)(x)\n",
    "    x = Bidirectional(GRU(80, return_sequences=True))(x)\n",
    "    avg_pool = GlobalAveragePooling1D()(x)\n",
    "    max_pool = GlobalMaxPooling1D()(x)\n",
    "    conc = concatenate([avg_pool, max_pool])\n",
    "    outp = Dense(6, activation=\"sigmoid\")(conc)\n",
    "    \n",
    "    model = Model(inputs=inp, outputs=outp)\n",
    "    model.compile(loss='binary_crossentropy',\n",
    "                  optimizer='adam',\n",
    "                  metrics=['accuracy'])\n",
    "\n",
    "    return model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "class RocAucEvaluation(Callback):\n",
    "    def __init__(self, validation_data=(), interval=1):\n",
    "        super(Callback, self).__init__()\n",
    "\n",
    "        self.interval = interval\n",
    "        self.X_val, self.y_val = validation_data\n",
    "\n",
    "    def on_epoch_end(self, epoch, logs={}):\n",
    "        if epoch % self.interval == 0:\n",
    "            y_pred = self.model.predict(self.X_val, verbose=0)\n",
    "            score = roc_auc_score(self.y_val, y_pred)\n",
    "            print(\"\\n ROC-AUC - epoch: %d - score: %.6f \\n\" % (epoch+1, score))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def createModel():\n",
    "    model = get_model()\n",
    "    batch_size = 32\n",
    "    epochs=2\n",
    "    model.fit(X_train, y, epochs=epochs, batch_size=batch_size, validation_split=0.1)\n",
    "    return model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Fit model if doesn't exist already "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start fitting...\n",
      "('Model already exists. Loading from path ', 'pooledgru_fasttext_model_v2.h5')\n"
     ]
    }
   ],
   "source": [
    "print(\"start fitting...\")\n",
    "file_path = \"pooledgru_fasttext_model_v2.h5\"\n",
    "if os.path.isfile(file_path):\n",
    "    print (\"Model already exists. Loading from path \", file_path)\n",
    "    model = load_model(file_path)\n",
    "else:\n",
    "    print (\"Model doesn't exist already, training model and saving at path \", file_path)\n",
    "    model = createModel()\n",
    "    model.save(file_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "start predicting...\n",
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"start predicting...\")\n",
    "y_pred = model.predict(X_test, batch_size=1024)\n",
    "print (\"done\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "submission = pd.DataFrame.from_dict({'id': test['id']})\n",
    "for idx, col in enumerate(list_classes):\n",
    "    submission[col] = y_pred[:,idx]\n",
    "submission.to_csv('submission_pooled.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Calculate AUC loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "from score import calc_auc_score, calc_log_loss"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "def get_scores(test, preds, fallback_preds_filename):\n",
    "    try: \n",
    "        true = test\n",
    "    except NameError:\n",
    "        true = pd.read_csv('../dataset/test_new.csv')\n",
    "    try: \n",
    "        y_pred = preds\n",
    "    except NameError:\n",
    "        pred = pd.read_csv(fallback_preds_filename)\n",
    "        y_pred = pred[list_classes].values\n",
    "\n",
    "    y_true = true[list_classes].values\n",
    "\n",
    "    loss = calc_log_loss(y_true, y_pred)\n",
    "    auc = calc_auc_score(y_true, y_pred)\n",
    "    return loss, auc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "true = pd.read_csv('./dataset/test_new.csv')\n",
    "pred = y_pred\n",
    "\n",
    "loss_, aucs = get_scores(true, pred, fallback_preds_filename=None)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Log loss =  0.041929289638716\n",
      "AUC Score =  0.9878130862611308\n"
     ]
    }
   ],
   "source": [
    "print (\"Log loss = \", loss_)\n",
    "print (\"AUC Score = \", aucs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Demo"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 77,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[0.06863956, 0.01733674, 0.03352913, 0.00464096, 0.03596547,\n",
       "        0.01597567]], dtype=float32)"
      ]
     },
     "execution_count": 77,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "zgg = tokenizer.texts_to_sequences([\"Not the brightest crayon in the box now, are we?\"])\n",
    "aaa = sequence.pad_sequences(zgg, maxlen=200)\n",
    "np.set_printoptions(suppress=True)\n",
    "model.predict(aaa)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 79,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "__________________________________________________________________________________________________\n",
      "Layer (type)                    Output Shape         Param #     Connected to                     \n",
      "==================================================================================================\n",
      "input_1 (InputLayer)            (None, 200)          0                                            \n",
      "__________________________________________________________________________________________________\n",
      "embedding_1 (Embedding)         (None, 200, 300)     6000000     input_1[0][0]                    \n",
      "__________________________________________________________________________________________________\n",
      "spatial_dropout1d_1 (SpatialDro (None, 200, 300)     0           embedding_1[0][0]                \n",
      "__________________________________________________________________________________________________\n",
      "bidirectional_1 (Bidirectional) (None, 200, 160)     182880      spatial_dropout1d_1[0][0]        \n",
      "__________________________________________________________________________________________________\n",
      "global_average_pooling1d_1 (Glo (None, 160)          0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "global_max_pooling1d_1 (GlobalM (None, 160)          0           bidirectional_1[0][0]            \n",
      "__________________________________________________________________________________________________\n",
      "concatenate_1 (Concatenate)     (None, 320)          0           global_average_pooling1d_1[0][0] \n",
      "                                                                 global_max_pooling1d_1[0][0]     \n",
      "__________________________________________________________________________________________________\n",
      "dense_1 (Dense)                 (None, 6)            1926        concatenate_1[0][0]              \n",
      "==================================================================================================\n",
      "Total params: 6,184,806\n",
      "Trainable params: 6,184,806\n",
      "Non-trainable params: 0\n",
      "__________________________________________________________________________________________________\n"
     ]
    }
   ],
   "source": [
    "model.summary()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "from keras.utils import plot_model\n",
    "plot_model(model, to_file='model.png')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "model = get_model()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/svg+xml": [
       "<svg height=\"483pt\" viewBox=\"0.00 0.00 649.77 483.00\" width=\"650pt\" xmlns=\"http://www.w3.org/2000/svg\" xmlns:xlink=\"http://www.w3.org/1999/xlink\">\n",
       "<g class=\"graph\" id=\"graph0\" transform=\"scale(1 1) rotate(0) translate(4 479)\">\n",
       "<title>G</title>\n",
       "<polygon fill=\"#ffffff\" points=\"-4,4 -4,-479 645.7739,-479 645.7739,4 -4,4\" stroke=\"transparent\"/>\n",
       "<!-- 4577721808 -->\n",
       "<g class=\"node\" id=\"node1\">\n",
       "<title>4577721808</title>\n",
       "<polygon fill=\"none\" points=\"266.7393,-438.5 266.7393,-474.5 395.1016,-474.5 395.1016,-438.5 266.7393,-438.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"330.9204\" y=\"-452.3\">input_1: InputLayer</text>\n",
       "</g>\n",
       "<!-- 4452862608 -->\n",
       "<g class=\"node\" id=\"node2\">\n",
       "<title>4452862608</title>\n",
       "<polygon fill=\"none\" points=\"248.8496,-365.5 248.8496,-401.5 412.9912,-401.5 412.9912,-365.5 248.8496,-365.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"330.9204\" y=\"-379.3\">embedding_1: Embedding</text>\n",
       "</g>\n",
       "<!-- 4577721808&#45;&gt;4452862608 -->\n",
       "<g class=\"edge\" id=\"edge1\">\n",
       "<title>4577721808-&gt;4452862608</title>\n",
       "<path d=\"M330.9204,-438.4551C330.9204,-430.3828 330.9204,-420.6764 330.9204,-411.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"334.4205,-411.5903 330.9204,-401.5904 327.4205,-411.5904 334.4205,-411.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4577722128 -->\n",
       "<g class=\"node\" id=\"node3\">\n",
       "<title>4577722128</title>\n",
       "<polygon fill=\"none\" points=\"210.3496,-292.5 210.3496,-328.5 451.4912,-328.5 451.4912,-292.5 210.3496,-292.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"330.9204\" y=\"-306.3\">spatial_dropout1d_1: SpatialDropout1D</text>\n",
       "</g>\n",
       "<!-- 4452862608&#45;&gt;4577722128 -->\n",
       "<g class=\"edge\" id=\"edge2\">\n",
       "<title>4452862608-&gt;4577722128</title>\n",
       "<path d=\"M330.9204,-365.4551C330.9204,-357.3828 330.9204,-347.6764 330.9204,-338.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"334.4205,-338.5903 330.9204,-328.5904 327.4205,-338.5904 334.4205,-338.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4452863312 -->\n",
       "<g class=\"node\" id=\"node4\">\n",
       "<title>4452863312</title>\n",
       "<polygon fill=\"none\" points=\"199.8701,-219.5 199.8701,-255.5 461.9707,-255.5 461.9707,-219.5 199.8701,-219.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"330.9204\" y=\"-233.3\">bidirectional_1(gru_1): Bidirectional(GRU)</text>\n",
       "</g>\n",
       "<!-- 4577722128&#45;&gt;4452863312 -->\n",
       "<g class=\"edge\" id=\"edge3\">\n",
       "<title>4577722128-&gt;4452863312</title>\n",
       "<path d=\"M330.9204,-292.4551C330.9204,-284.3828 330.9204,-274.6764 330.9204,-265.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"334.4205,-265.5903 330.9204,-255.5904 327.4205,-265.5904 334.4205,-265.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4578653520 -->\n",
       "<g class=\"node\" id=\"node5\">\n",
       "<title>4578653520</title>\n",
       "<polygon fill=\"none\" points=\"0,-146.5 0,-182.5 331.8408,-182.5 331.8408,-146.5 0,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"165.9204\" y=\"-160.3\">global_average_pooling1d_1: GlobalAveragePooling1D</text>\n",
       "</g>\n",
       "<!-- 4452863312&#45;&gt;4578653520 -->\n",
       "<g class=\"edge\" id=\"edge4\">\n",
       "<title>4452863312-&gt;4578653520</title>\n",
       "<path d=\"M290.1339,-219.4551C267.7237,-209.5403 239.741,-197.16 216.0115,-186.6615\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"217.3707,-183.4356 206.8096,-182.5904 214.5384,-189.8371 217.3707,-183.4356\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4578654992 -->\n",
       "<g class=\"node\" id=\"node6\">\n",
       "<title>4578654992</title>\n",
       "<polygon fill=\"none\" points=\"350.0669,-146.5 350.0669,-182.5 641.7739,-182.5 641.7739,-146.5 350.0669,-146.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"495.9204\" y=\"-160.3\">global_max_pooling1d_1: GlobalMaxPooling1D</text>\n",
       "</g>\n",
       "<!-- 4452863312&#45;&gt;4578654992 -->\n",
       "<g class=\"edge\" id=\"edge5\">\n",
       "<title>4452863312-&gt;4578654992</title>\n",
       "<path d=\"M371.7069,-219.4551C394.1171,-209.5403 422.0998,-197.16 445.8293,-186.6615\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"447.3024,-189.8371 455.0312,-182.5904 444.4702,-183.4356 447.3024,-189.8371\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4577720016 -->\n",
       "<g class=\"node\" id=\"node7\">\n",
       "<title>4577720016</title>\n",
       "<polygon fill=\"none\" points=\"244.6011,-73.5 244.6011,-109.5 417.2397,-109.5 417.2397,-73.5 244.6011,-73.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"330.9204\" y=\"-87.3\">concatenate_1: Concatenate</text>\n",
       "</g>\n",
       "<!-- 4578653520&#45;&gt;4577720016 -->\n",
       "<g class=\"edge\" id=\"edge6\">\n",
       "<title>4578653520-&gt;4577720016</title>\n",
       "<path d=\"M206.7069,-146.4551C229.1171,-136.5403 257.0998,-124.16 280.8293,-113.6615\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"282.3024,-116.8371 290.0312,-109.5904 279.4702,-110.4356 282.3024,-116.8371\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4578654992&#45;&gt;4577720016 -->\n",
       "<g class=\"edge\" id=\"edge7\">\n",
       "<title>4578654992-&gt;4577720016</title>\n",
       "<path d=\"M455.1339,-146.4551C432.7237,-136.5403 404.741,-124.16 381.0115,-113.6615\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"382.3707,-110.4356 371.8096,-109.5904 379.5384,-116.8371 382.3707,-110.4356\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "<!-- 4591150736 -->\n",
       "<g class=\"node\" id=\"node8\">\n",
       "<title>4591150736</title>\n",
       "<polygon fill=\"none\" points=\"278.7944,-.5 278.7944,-36.5 383.0464,-36.5 383.0464,-.5 278.7944,-.5\" stroke=\"#000000\"/>\n",
       "<text fill=\"#000000\" font-family=\"Times,serif\" font-size=\"14.00\" text-anchor=\"middle\" x=\"330.9204\" y=\"-14.3\">dense_1: Dense</text>\n",
       "</g>\n",
       "<!-- 4577720016&#45;&gt;4591150736 -->\n",
       "<g class=\"edge\" id=\"edge8\">\n",
       "<title>4577720016-&gt;4591150736</title>\n",
       "<path d=\"M330.9204,-73.4551C330.9204,-65.3828 330.9204,-55.6764 330.9204,-46.6817\" fill=\"none\" stroke=\"#000000\"/>\n",
       "<polygon fill=\"#000000\" points=\"334.4205,-46.5903 330.9204,-36.5904 327.4205,-46.5904 334.4205,-46.5903\" stroke=\"#000000\"/>\n",
       "</g>\n",
       "</g>\n",
       "</svg>"
      ],
      "text/plain": [
       "<IPython.core.display.SVG object>"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:py3]",
   "language": "python",
   "name": "conda-env-py3-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
